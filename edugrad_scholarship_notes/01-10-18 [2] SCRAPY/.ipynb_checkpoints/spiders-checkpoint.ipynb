{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Spiders \n",
    "\n",
    "Spiders are custom classes where you define custom behavior to scrape information from websites and that Scrapy uses to scrape information from a website (or a group of websites).  \n",
    "\n",
    "## How to create a spider?\n",
    "\n",
    "1) Well, spiders are only classes in a python file , therefore you have to create a simple python file with one class. but before that, you have to create a project in which `scrapy` had already seeted up the extra files/environment needed to scrap a website. That is, you need to first set up a `Project`.    \n",
    "we get this project by `cd` into the repository you want and then running the command `scrapy genspider <projectname>\n",
    "\n",
    "2) Next we get into the project where we see a lot of files with one specific `spiders` directory. cd `spiders`.  \n",
    "\n",
    "3) Here is the place where we create our  first spider file. create a python file and give it a name.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contents of the spider file.\n",
    "\n",
    "a sample spider file looks like the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "class IntroSpider(scrapy.Spider):\n",
    "    name = \"intro_spider\" \n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            \"http://quotes.toscrape.com/page/1/\",\n",
    "            \"http://quotes.toscrape.com/page/2/\"\n",
    "             ]\n",
    "        for i in urls :\n",
    "            yield scrapy.Request(url=i,callback=self.parse)\n",
    "            \n",
    "    def parse(self,response):\n",
    "        filename = \"quotes-list.txt\"\n",
    "        \n",
    "        quotes_list = response.css(\".text::text\").extract()\n",
    "        author_list = response.css(\".author::text\").extract()\n",
    "        \n",
    "        with open(filename,'a+') as f:\n",
    "            for quote,author in zip(quotes_list,author_list):\n",
    "                f.write(quote+','+author+'\\n')\n",
    "\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's understand each row's meaning.  \n",
    "\n",
    "\n",
    "1. `import scrapy`  : self explainatory.\n",
    "2. `class IntroSpider(scrapy.Spider)` : we have to create a class of any name that extends scrapy.Spider , in order to be recognisable by scrapy.  \n",
    "3. `name = \"intro_spider\"` : this is an important and must have statement for every spider. scrapy recognises your spider not by its class name , but by the value present in `name` variable.(i don't know about the character-set limitations, but try to use general character sets, like alphabets, numerals and _ )\n",
    "4.   \n",
    "\n",
    "```\n",
    "5      def start_requests(self):\n",
    "6          urls = [\n",
    "7              \"http://quotes.toscrape.com/page/1/\",\n",
    "8              \"http://quotes.toscrape.com/page/2/\"\n",
    "9              ]\n",
    "10         for i in urls :\n",
    "11            yield scrapy.Request(url=i,callback=self.parse)\n",
    "12          \n",
    "\n",
    "```\n",
    "\n",
    "this function returns a list of urls in a generator manner. this is called internally to get a list of urls to be scraped and getting to know the function to be called for that perticular url\n",
    "\n",
    "5.  \n",
    "\n",
    "```\n",
    "13    def parse(self,response):\n",
    "14        filename = \"quotes-list.txt\"\n",
    "15\n",
    "16        quotes_list = response.css(\".text::text\").extract()\n",
    "17        author_list = response.css(\".author::text\").extract()\n",
    "18\n",
    "19        with open(filename,'a+') as f:\n",
    "20            for quote,author in zip(quotes_list,author_list):\n",
    "21                f.write(quote+','+author+'\\n')\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "This function is the response function that will be called after complete parsing of page.\n",
    "we define it by ourself , but it needs to follow a psecific syntax i.e:  \n",
    "`def func_name(self, response)--> None`  \n",
    "Here we can manipulate the  response recieved. using the xpath/css functions used previously in\n",
    "Introduction.\n",
    "This program will create a list of all the quotes and authors and will save it into a file named\n",
    "\"quotes-list.txt\".\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "well, that's it!When we run the command:  \n",
    "```\n",
    "scrapy crawl intro_spider\n",
    "```\n",
    "\n",
    "scrapy crawls the website, and runs the `parse(..)` function with the recieved response , and therby we have a text file with all the quotes and authors. it is very fast in nature.\n",
    "Note that the terminal will throw errors if their is 404 page not found or missing internet problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
