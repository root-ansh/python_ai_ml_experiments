{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scrapy\n",
    "Scrapy is a fast, open-source web crawling framework written in Python, used to extract the data from the web page with the help of selectors based on XPath/ Css classes.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "1. Install anaconda\n",
    "2. open anaconda terminal in adminstrator from ` windows>(right click) anaconda-prompt>> open in adminstrator -mode  \n",
    "3.  `pip install scrapy`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with the scrapy shell\n",
    "\n",
    "Scrapy shell has a lot of powerful features to try and experiment on the go . you can checkout more of the experimenting here :  \n",
    "- zzzspider.txt\n",
    "- zzzspider2.txt\n",
    "- spider.docx   \n",
    "\n",
    "\n",
    "so defining some important commands briefly,:  \n",
    "\n",
    "0. `scrapy shell` : write this in your command line to open scrapy shell in comd, a python's IDE like interpreter for scrapy.  \n",
    "1.  `scrapy`      :scrapy module (contains scrapy.Request, scrapy.Selector, etc){haven't used this in shell till now}\n",
    "2.   crawler     :<scrapy.crawler.Crawler object at 0x000001E5367C9C50>{haven't used this in shell till now}\n",
    "3.   item        :{}{haven't used this in shell till now}\n",
    "4.   settings    :<scrapy.settings.Settings object at 0x000001E538EE0908>{haven't used this in shell till now}\n",
    "\n",
    "5.   fetch(url[, redirect=True]) : Fetch URL and update local objects (by default, redirects are followed)\n",
    "6.   fetch(req)                  : Fetch a scrapy.Request and update local objects. creates a 'response' object\n",
    "7.   shelp()           \t  : Shell help (print this help)\n",
    "8.   response\t\t    : a string like object. which holdes the complete data after scrapy has scratched the complete website.print this to see every possible tag/css of the scraped website\n",
    "9.   view(response)    \t  : View the downloaded response in a browser. this will be fairly different than the actual website because scrapy **does not** scrapes dynamic data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## default settings for  scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings={'AJAXCRAWL_ENABLED': False, 'AUTOTHROTTLE_DEBUG': False, 'AUTOTHROTTLE_ENABLED': False, 'AUTOTHROTTLE_MAX_DELAY': 60.0,\n",
    " 'AUTOTHROTTLE_START_DELAY': 5.0, 'AUTOTHROTTLE_TARGET_CONCURRENCY': 1.0, 'BOT_NAME': 'scrapybot',\n",
    " 'CLOSESPIDER_ERRORCOUNT': 0,\n",
    " 'CLOSESPIDER_ITEMCOUNT': 0, 'CLOSESPIDER_PAGECOUNT': 0, 'CLOSESPIDER_TIMEOUT': 0, 'COMMANDS_MODULE': '',\n",
    " 'COMPRESSION_ENABLED': True,\n",
    " 'CONCURRENT_ITEMS': 100, 'CONCURRENT_REQUESTS': 16, 'CONCURRENT_REQUESTS_PER_DOMAIN': 8,\n",
    " 'CONCURRENT_REQUESTS_PER_IP': 0, 'COOKIES_DEBUG': False,\n",
    " 'COOKIES_ENABLED': True, 'DEFAULT_ITEM_CLASS': 'scrapy.item.Item',\n",
    " 'DEFAULT_REQUEST_HEADERS': {'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "                             'Accept-Language': 'en'},\n",
    " 'DEPTH_LIMIT': 0, 'DEPTH_PRIORITY': 0, 'DEPTH_STATS': True, 'DNSCACHE_ENABLED': True, 'DNSCACHE_SIZE': 10000,\n",
    " 'DNS_TIMEOUT': 60,\n",
    " 'DOWNLOADER': 'scrapy.core.downloader.Downloader',\n",
    " 'DOWNLOADER_CLIENTCONTEXTFACTORY': 'scrapy.core.downloader.contextfactory.ScrapyClientContextFactory',\n",
    " 'DOWNLOADER_CLIENT_TLS_METHOD': 'TLS',\n",
    " 'DOWNLOADER_HTTPCLIENTFACTORY': 'scrapy.core.downloader.webclient.ScrapyHTTPClientFactory',\n",
    " 'DOWNLOADER_MIDDLEWARES': {},\n",
    " 'DOWNLOADER_MIDDLEWARES_BASE': {'scrapy.downloadermiddlewares.ajaxcrawl.AjaxCrawlMiddleware': 560,\n",
    "                                 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware': 700,\n",
    "                                 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware': 400,\n",
    "                                 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware': 350,\n",
    "                                 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware': 300,\n",
    "                                 'scrapy.downloadermiddlewares.httpcache.HttpCacheMiddleware': 900,\n",
    "                                 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware': 590,\n",
    "                                 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware': 750,\n",
    "                                 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware': 580,\n",
    "                                 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware': 600,\n",
    "                                 'scrapy.downloadermiddlewares.retry.RetryMiddleware': 550,\n",
    "                                 'scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware': 100,\n",
    "                                 'scrapy.downloadermiddlewares.stats.DownloaderStats': 850,\n",
    "                                 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': 500},\n",
    " 'DOWNLOADER_STATS': True, 'DOWNLOAD_DELAY': 0, 'DOWNLOAD_FAIL_ON_DATALOSS': True, 'DOWNLOAD_HANDLERS': {},\n",
    " 'DOWNLOAD_HANDLERS_BASE': {'data': 'scrapy.core.downloader.handlers.datauri.DataURIDownloadHandler',\n",
    "                            'file': 'scrapy.core.downloader.handlers.file.FileDownloadHandler',\n",
    "                            'ftp': 'scrapy.core.downloader.handlers.ftp.FTPDownloadHandler',\n",
    "                            'http': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',\n",
    "                            'https': 'scrapy.core.downloader.handlers.http.HTTPDownloadHandler',\n",
    "                            's3': 'scrapy.core.downloader.handlers.s3.S3DownloadHandler'},\n",
    " 'DOWNLOAD_MAXSIZE': 1073741824, 'DOWNLOAD_TIMEOUT': 180, 'DOWNLOAD_WARNSIZE': 33554432,\n",
    " 'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter', 'EDITOR': '%s -m idlelib.idle', 'EXTENSIONS': {},\n",
    " 'EXTENSIONS_BASE': {'scrapy.extensions.closespider.CloseSpider': 0, 'scrapy.extensions.corestats.CoreStats': 0,\n",
    "                     'scrapy.extensions.feedexport.FeedExporter': 0, 'scrapy.extensions.logstats.LogStats': 0,\n",
    "                     'scrapy.extensions.memdebug.MemoryDebugger': 0, 'scrapy.extensions.memusage.MemoryUsage': 0,\n",
    "                     'scrapy.extensions.spiderstate.SpiderState': 0, 'scrapy.extensions.telnet.TelnetConsole': 0,\n",
    "                     'scrapy.extensions.throttle.AutoThrottle': 0}, 'FEED_EXPORTERS': {},\n",
    " 'FEED_EXPORTERS_BASE': {'csv': 'scrapy.exporters.CsvItemExporter', 'jl': 'scrapy.exporters.JsonLinesItemExporter',\n",
    "                         'json': 'scrapy.exporters.JsonItemExporter',\n",
    "                         'jsonlines': 'scrapy.exporters.JsonLinesItemExporter',\n",
    "                         'marshal': 'scrapy.exporters.MarshalItemExporter',\n",
    "                         'pickle': 'scrapy.exporters.PickleItemExporter', 'xml': 'scrapy.exporters.XmlItemExporter'},\n",
    " 'FEED_EXPORT_ENCODING': None, 'FEED_EXPORT_FIELDS': None, 'FEED_EXPORT_INDENT': 0, 'FEED_FORMAT': 'jsonlines',\n",
    " 'FEED_STORAGES': {}, 'FEED_STORAGES_BASE': {'': 'scrapy.extensions.feedexport.FileFeedStorage',\n",
    "                                             'file': 'scrapy.extensions.feedexport.FileFeedStorage',\n",
    "                                             'ftp': 'scrapy.extensions.feedexport.FTPFeedStorage',\n",
    "                                             's3': 'scrapy.extensions.feedexport.S3FeedStorage',\n",
    "                                             'stdout': 'scrapy.extensions.feedexport.StdoutFeedStorage'},\n",
    " 'FEED_STORE_EMPTY': False, 'FEED_TEMPDIR': None, 'FEED_URI': None, 'FEED_URI_PARAMS': None,\n",
    " 'FILES_STORE_S3_ACL': 'private', 'FTP_PASSIVE_MODE': True, 'FTP_PASSWORD': 'guest', 'FTP_USER': 'anonymous',\n",
    " 'HTTPCACHE_ALWAYS_STORE': False, 'HTTPCACHE_DBM_MODULE': 'dbm', 'HTTPCACHE_DIR': 'httpcache',\n",
    " 'HTTPCACHE_ENABLED': False, 'HTTPCACHE_EXPIRATION_SECS': 0, 'HTTPCACHE_GZIP': False, 'HTTPCACHE_IGNORE_HTTP_CODES': [],\n",
    " 'HTTPCACHE_IGNORE_MISSING': False, 'HTTPCACHE_IGNORE_RESPONSE_CACHE_CONTROLS': [],\n",
    " 'HTTPCACHE_IGNORE_SCHEMES': ['file'], 'HTTPCACHE_POLICY': 'scrapy.extensions.httpcache.DummyPolicy',\n",
    " 'HTTPCACHE_STORAGE': 'scrapy.extensions.httpcache.FilesystemCacheStorage', 'HTTPPROXY_AUTH_ENCODING': 'latin-1',\n",
    " 'HTTPPROXY_ENABLED': True, 'IMAGES_STORE_S3_ACL': 'private', 'ITEM_PIPELINES': {}, 'ITEM_PIPELINES_BASE': {},\n",
    " 'ITEM_PROCESSOR': 'scrapy.pipelines.ItemPipelineManager', 'KEEP_ALIVE': True, 'LOGSTATS_INTERVAL': 0,\n",
    " 'LOG_DATEFORMAT': '%Y-%m-%d %H:%M:%S', 'LOG_ENABLED': True, 'LOG_ENCODING': 'utf-8', 'LOG_FILE': None,\n",
    " 'LOG_FORMAT': '%(asctime)s [%(name)s] %(levelname)s: %(message)s', 'LOG_FORMATTER': 'scrapy.logformatter.LogFormatter',\n",
    " 'LOG_LEVEL': 'DEBUG', 'LOG_SHORT_NAMES': False, 'LOG_STDOUT': False, 'MAIL_FROM': 'scrapy@localhost',\n",
    " 'MAIL_HOST': 'localhost', 'MAIL_PASS': None, 'MAIL_PORT': 25, 'MAIL_USER': None, 'MEMDEBUG_ENABLED': False,\n",
    " 'MEMDEBUG_NOTIFY': [], 'MEMUSAGE_CHECK_INTERVAL_SECONDS': 60.0, 'MEMUSAGE_ENABLED': True, 'MEMUSAGE_LIMIT_MB': 0,\n",
    " 'MEMUSAGE_NOTIFY_MAIL': [], 'MEMUSAGE_WARNING_MB': 0, 'METAREFRESH_ENABLED': True, 'METAREFRESH_MAXDELAY': 100,\n",
    " 'NEWSPIDER_MODULE': '', 'RANDOMIZE_DOWNLOAD_DELAY': True, 'REACTOR_THREADPOOL_MAXSIZE': 10, 'REDIRECT_ENABLED': True,\n",
    " 'REDIRECT_MAX_TIMES': 20, 'REDIRECT_PRIORITY_ADJUST': 2, 'REFERER_ENABLED': True,\n",
    " 'REFERRER_POLICY': 'scrapy.spidermiddlewares.referer.DefaultReferrerPolicy', 'RETRY_ENABLED': True,\n",
    " 'RETRY_HTTP_CODES': [500, 502, 503, 504, 522, 524, 408], 'RETRY_PRIORITY_ADJUST': -1, 'RETRY_TIMES': 2,\n",
    " 'ROBOTSTXT_OBEY': False, 'SCHEDULER': 'scrapy.core.scheduler.Scheduler', 'SCHEDULER_DEBUG': False,\n",
    " 'SCHEDULER_DISK_QUEUE': 'scrapy.squeues.PickleLifoDiskQueue',\n",
    " 'SCHEDULER_MEMORY_QUEUE': 'scrapy.squeues.LifoMemoryQueue', 'SCHEDULER_PRIORITY_QUEUE': 'queuelib.PriorityQueue',\n",
    " 'SPIDER_CONTRACTS': {},\n",
    " 'SPIDER_CONTRACTS_BASE': {'scrapy.contracts.default.ReturnsContract': 2, 'scrapy.contracts.default.ScrapesContract': 3,\n",
    "                           'scrapy.contracts.default.UrlContract': 1},\n",
    " 'SPIDER_LOADER_CLASS': 'scrapy.spiderloader.SpiderLoader', 'SPIDER_LOADER_WARN_ONLY': False, 'SPIDER_MIDDLEWARES': {},\n",
    " 'SPIDER_MIDDLEWARES_BASE': {'scrapy.spidermiddlewares.depth.DepthMiddleware': 900,\n",
    "                             'scrapy.spidermiddlewares.httperror.HttpErrorMiddleware': 50,\n",
    "                             'scrapy.spidermiddlewares.offsite.OffsiteMiddleware': 500,\n",
    "                             'scrapy.spidermiddlewares.referer.RefererMiddleware': 700,\n",
    "                             'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware': 800}, 'SPIDER_MODULES': [],\n",
    " 'STATSMAILER_RCPTS': [], 'STATS_CLASS': 'scrapy.statscollectors.MemoryStatsCollector', 'STATS_DUMP': True,\n",
    " 'TELNETCONSOLE_ENABLED': 1, 'TELNETCONSOLE_HOST': '127.0.0.1', 'TELNETCONSOLE_PORT': [6023, 6073],\n",
    " 'TEMPLATES_DIR': 'C:\\\\ProgramData\\\\Anaconda3\\\\lib\\\\site-packages\\\\scrapy\\\\templates', 'URLLENGTH_LIMIT': 2083,\n",
    " 'USER_AGENT': 'Scrapy/1.5.1 (+https://scrapy.org)'}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## xpath and CSS selectors\n",
    "When you are scraping the web pages, you need to extract a certain part of the HTML source by using the mechanism called selectors, achieved by using either XPath or CSS expressions. Selectors are built upon the `lxml library` ,  which processes the XML and HTML in Python language.  \n",
    "\n",
    "methods:  \n",
    "- using xpath( nodes of html/css) \n",
    "- using css selection(selecting the html componenets styled with a perticular kind of css)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### using css selectors\n",
    "\n",
    "\n",
    "In `scrapy shell` , you can simply try out by fetching the url first(using `fetch(\"url\")` ) and then checking out the following commands:  \n",
    "```\n",
    "In [12]: response.css('title')\n",
    "Out[12]: [<Selector xpath='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]\n",
    "#basically it returned the list of  objects which internally selects the title(s)\n",
    "#{since <title > is only used once in the html, it shows a single element list}\n",
    "\n",
    "In [13]: response.css('title').extract()\n",
    "Out[13]: ['<title>Quotes to Scrape</title>']\n",
    "# extracted the tags directly\n",
    "\n",
    "In [15]: response.css('title::text').extract()\n",
    "Out[15]: ['Quotes to Scrape']\n",
    "#extracted the value of 'text' attribute of title\n",
    "\n",
    "```\n",
    "\n",
    "lets try a complex one:  \n",
    "\n",
    "```python\n",
    "In [31]: response.css('.footer > .container > p >a::attr(href)').extract()\n",
    "Out[31]: ['https://www.goodreads.com/quotes', 'https://scrapinghub.com']\n",
    "```\n",
    "*Interpretation:*\n",
    "- there is some HTML tag <xyz >styed with css class '.footer' .(we don't care what xyz is) . it will search for all '.footer' classes.\n",
    "- inside '.footer' class, it will search for all instances of  '.container'  classes.\n",
    "- inside '.container' class, it will look for all <p> tags. notice this time we are  looking for a <tag>, therefore it DOES matter what the tag is , but DOESN't matter what its class is.\n",
    "- inside every <p> tag , it will check , if some <a> tag present . if present, it will extract its value of attribute 'href' and store it in a list. \n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "In [34]: response.css('.footer > .container > p:nth-child(2) >a::attr(href)').extract()\n",
    "Out[34]: ['https://scrapinghub.com']\n",
    "# tag:nth-child(2) is similar to saying  2nd <p> tag out of all possible <p> tags\n",
    "\n",
    "``` \n",
    "\n",
    "thus, for css:  \n",
    "- **going inside a class     \t\t-> .classname**\n",
    "- **going inside a <tag >  \t\t-> tag**  \n",
    "- **getting value of an attribute of such tag\t-> tag::attr(attributename)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### USING XPATH \n",
    "Instead of using css designs and classes , we rather use html paths to crawl.\n",
    "```python\n",
    "In [13]: response.css('title').extract()\n",
    "Out[13]: ['<title>Quotes to Scrape</title>']\n",
    "\n",
    "In [39]: response.xpath('/html/head/title').extract()\n",
    "Out[39]: ['<title>Quotes to Scrape</title>']           \n",
    "\n",
    "#look at that!, its similar to  what we did using css!\n",
    "\n",
    "==================================================================\n",
    "\n",
    "In [40]: response.xpath('//title').extract() # incase you don't wanna provide a path or just search in whole html\n",
    "Out[40]: ['<title>Quotes to Scrape</title>']\n",
    "\n",
    "```\n",
    "\n",
    "we can get xpath by `ctrl+shift+i`(debugger tools)>> right clicking on the element again >>`copy`>>`copy xPath`.  \n",
    "\n",
    "```python\n",
    "In [43]: response.xpath('/html/body/div/div/div/div/span/small').extract()   \n",
    "Out[43]:\n",
    "['<small class=\"author\" itemprop=\"author\">Albert Einstein</small>',\n",
    " '<small class=\"author\" itemprop=\"author\">J.K. Rowling</small>',\n",
    " '<small class=\"author\" itemprop=\"author\">Albert Einstein</small>',\n",
    " '<small class=\"author\" itemprop=\"author\">Jane Austen</small>',\n",
    " '<small class=\"author\" itemprop=\"author\">Marilyn Monroe</small>',\n",
    "...\n",
    "]\n",
    "\n",
    "In [43]: response.xpath('/html/body/div/div/div/div/span/small/text()').extract()    \n",
    "out[43]:[\n",
    "Albert Einstein',\n",
    " 'J.K. Rowling',\n",
    " 'Albert Einstein',\n",
    " 'Jane Austen',\n",
    " 'Marilyn Monroe',\n",
    " 'Albert Einstein',\n",
    ".......]\n",
    "\n",
    "In [45]: response.xpath(\"//span[@class='text']\").extract()\n",
    "Out[45]:\n",
    "['<span class=\"text\" itemprop=\"text\">“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”</span>',\n",
    " '<span class=\"text\" itemprop=\"text\">“It is our choices, Harry, that show what we truly are, far more than our abilities.”</span>',\n",
    " '<span class=\"text\" itemprop=\"text\">“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”</span>',\n",
    "...\n",
    "]\n",
    "# interpretation : returns the data of \"all <span> tags having class 'text'\n",
    "\n",
    "\n",
    "In [46]: response.xpath(\"//span[@class='text']/text()\").extract()\n",
    "Out[46]:\n",
    "['“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”',\n",
    " '“It is our choices, Harry, that show what we truly are, far more than our abilities.”',\n",
    " '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”',\n",
    " '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”',\n",
    " \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\",\n",
    "......................]\n",
    "\n",
    "```\n",
    "\n",
    "thus ,In general for xpath, if we have to get value of an attribute `'text()'` of tag  `'<TAG>'` and class `'css_class'` :  \n",
    "**`>> response.xpath(\"//TAG[@class='css_class']/text()\").extract()`**\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can use regular expressions for searching too.  \n",
    "### regular expressions for  xpath and css\n",
    "\n",
    "\n",
    "```jupyter\n",
    "\n",
    "In [53]: response.xpath(\"//*[contains(text(),'love')]/text()\")\n",
    "Out[53]:\n",
    "[<Selector xpath=\"//*[contains(text(),'love')]/text()\" data='“This life is what you make it. No matte'>,\n",
    " <Selector xpath=\"//*[contains(text(),'love')]/text()\" data='love'>,\n",
    " <Selector xpath=\"//*[contains(text(),'love')]/text()\" data='“You may not be her first, her last, or '>,\n",
    " <Selector xpath=\"//*[contains(text(),'love')]/text()\" data='love'>,\n",
    " <Selector xpath=\"//*[contains(text(),'love')]/text()\" data=\"“The opposite of love is not hate, it's \">,\n",
    " <Selector xpath=\"//*[contains(text(),'love')]/text()\" data='love'>,\n",
    " <Selector xpath=\"//*[contains(text(),'love')]/text()\" data='“It is not a lack of love, but a lack of'>,\n",
    " <Selector xpath=\"//*[contains(text(),'love')]/text()\" data='lack-of-love'>,\n",
    "...........]\n",
    "# selects all the <tags> whose `text()` attribute contains the word `'love'`\n",
    "\n",
    "css can be used in a manner like that too\n",
    "\n",
    "In [64]: response.css(\".text:contains('love')::text\").extract()\n",
    "Out[64]:\n",
    "[\"“This life is what you make it. No matter what, you're going to mess up sometimes, it's a universal truth. But the good part is you get to decide how you're going to mess it up. Girls will be your friends - they'll act like it anyway. But just remember, some come, some go. The ones that stay with you through everything - they're your true best friends. Don't let go of them.\n",
    "...]\n",
    "# selects and displays strings of all tags with class 'text' which contains text :love\n",
    "\n",
    "\n",
    "\n",
    "In [65]: response.css(\".author::text\").re('A[a-z]*\\s\\w+')\n",
    "Out[65]: ['Albert Einstein', 'Allen Saunders']\n",
    "# does to select all authoor class's text starting with 'A' and having space between.\n",
    "\n",
    "In [67]: response.xpath(\"//*[@class='author'][starts-with(text(),'A')]/text()\").extract()\n",
    "Out[67]: ['Albert Einstein', 'Allen Saunders']\n",
    "\n",
    "# does to select all authoor class's text starting with 'A' and having space between.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
